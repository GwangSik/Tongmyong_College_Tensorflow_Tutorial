<!-- $theme: default -->

Recurrent Neural Network
===

https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial

Created by **Choelhui lee**

---
<!-- page_number: true -->

# Recurrent Neural Network (RNN)


- **`Recurrent == 재귀적`** 
- `사전 의미` : **`[명사] 원래의 자리로 되돌아가거나 되돌아옴.`**

---

**일반적인 Nerual Network**

<center>

![일반적인 Nerual Network](https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg)

</center>

- **`각각의 입력이 독립적이기 때문에 서로에게 영향을 안줌!!`** 

---

- **`그러나, 입력 데이터간 연관성이 있는 정보들을 무수히 많음`**
- *example*:
	+ `Natural language` : 순차적으로 입력되는 정보간의 상관관계를 처리해야됨
	+ `voice` : 순차적으로 입력되는 정보간의 상관관계를 처리해야됨


# **`RNN이 해결책!!`**

---

## RNN

![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/01.%20RNN.PNG?raw=true)

- **`외부입력`과  `자신의 이전 상태`를 입력 받아 `자신의 상태를 갱신`**
- 모든 시간대에서 `동일한 매개변수(parameter)를 적용`


---

- `RNN을 Unfold 해보면...`

![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/02.%20RNN.PNG?raw=true)

---

![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/03.%20RNN.PNG?raw=true)

---

![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/04.%20RNN.PNG?raw=true)

---

![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/05.%20RNN.PNG?raw=true)

---

- **`풀고자하는 문제에 따라 RNN을 이용해 다양한 구조를 직접 설계`**

![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/07.%20RNN.PNG?raw=true)

---

## Backpropagation through time

- `RNN은 문장과 같은 sequence를 입력으로 받기 때문에 학습 시 backpropagation을` **`시간에 대해서도 수행`**
- `매 스텝마다 loss를 계산하며(E0~E4), 총 loss는 각 스텝 loss의 합`

<center>
  
![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/08.%20RNN.PNG?raw=true)

</center>

---

<center>
  
![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/09.%20RNN.PNG?raw=true)

</center>

---

## Truncated BPTT

<center>
  
![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/10.%20RNN.PNG?raw=true)

</center>

---

## RNN Problem (Vanishing gradients)

<center>
  
![RNN](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/11.%20RNN.PNG?raw=true)

</center>

+ **`RNN은 long-time sequence 를 잘 처리하지 못함`**
  - `여러 스텝이 지난 경우 먼 step의 단어를 까먹음`
  - **`각 스텝마다 그라이언트를 곱하면서 vanishing이 일어나기 때문!!`**

---

+ **해결책**
  - `RNN의 활성화 함수를 ReLU로 바꾸자 (하책)`
  - **`LSTM, GRU와 같은 RNN변형 모델을 사용해 보자 (상책)`**

---

# LSTM

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/12%20LSTM.PNG?raw=true)
 > **`기본 RNN보다 구조가 훨씬 복잡해보임`**

</center>

---

## LSTM 의 핵심

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/13.%20LSTM.PNG?raw=true)

</center>

- **`Cell State`**
   - 약간의 선형 상호작용만 일으키며 지나간다.
   - `일종의 컨베이어 벨트 역할` = **`과거의 데이터를 쭉~~~~ 알 수 있다!!`**
   		+  LSTM은 cell state에 정보를 더하거나 지운다.
   		+  `gate`들이 이 과정을 조절

---

**Gate(게이트) 란?**

- `sigmoid 함수에 의해 0~1로 제한`
- **`벡터와 게이트가 곱해진다면 이 벡터를 얼마나 통과시킬지 정해주는 역할`**

---

## LSTM Decomposition

- **`01. forget Gate`**
	+ `cell의 어떤 정보를 버릴 것인지 결정`
	+ `sigmoid로 결정`

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/14.%20LSTM.PNG?raw=true)

</center>

---

- **`02. Input Gate`**
	+ `sigmoid층은 어떤 값을 갱신할지 결정`
    + `tanh층은 cell state에 더해질 새로운 후보 값들의 벡터 Ct를 만듦`
    + `이 둘을 합침!!`

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/15.%20LSTM.PNG?raw=true)

</center>

---

- **`03. 갱신`**
	+ `낡은 Ct-1을 Ct로 갱신`

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/16.%20LSTM.PNG?raw=true)

</center>

---

- **`04. Output Gate`**
	+ `cell state 여과 버전`

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/17.%20LSTM.PNG?raw=true)

</center>

---

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/18..%20LSTM.PNG?raw=true)

</center>

---

### LSTM 파생형들

<center>
  
![LSTM](https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/19.%20LSTM.PNG?raw=true)

</center>

---

## Bi-RNN
- **`실제 우리가 글을 적거나 말을 할 때 앞의 나올 상황에서 뿐만 아니라 뒤에 상황도 고려`**
- 한쪽 방향 뿐만 아니라 **`양방향으로 학습 할 수 있게 설계`**
- 일반적으로 RNN보다 성능이 더 좋다고 알려져 짐
- 기본 RNN에서는 잘 사용하지 않고 `LSTM이나 GRU에서 많이 사용.`

<center>
  
<img src="https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/20.%20BI-RNN.PNG?raw=true" height="350" width="500">

</center>

---

## Multi-RNN

 - `RNN은 모든 sequence에 대해 계산하기 때문에 연산량이 NN, CNN보다 많아 CNN처럼 깊게 쌓기는 어려움`
 - `데이터가 많고 연산 속도가 빠른 환경이라면` **`Multi-RNN도 좋은 선택지가 될 수 있음`**
 - 기본 RNN에서는 잘 사용하지 않고 `LSTM이나 GRU에서 많이사용`

<center>

<img src="https://github.com/dlcjfgmlnasa/Tongmyong_College_Tensorflow_Tutorial/blob/master/img/03.%20RNN/21.%20Multi-RNN.PNG?raw=true" height="360" width="350">

</center>

---

# RNN 정리

- `문장과 같은 시퀸스 데이터를 처리할 때는 RNN계열의 네트워크를 고려 (가변 길이의 데이터를 입력 받을 수 있음)`
- `기본 RNN은 gradient vanishing / exploding 현상 우려`
- `LSTM, GRU는 기본 RNN의 장기 종속성 문제를 해결해 줌`

---




